{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "import pickle\n",
    "with open('data/reviews_clean.pkl', 'rb') as f:\n",
    "    reviews = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onestar = list(reviews[reviews['star_rating']==1]['review_text'])\n",
    "print(onestar[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules for Text Cleaning:\n",
    "- Short hand abbrev:\n",
    "    + bc -> because\n",
    "- Repeating words (sooo):\n",
    "    + regex??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special case rule\n",
    "from spacy.attrs import ORTH, LEMMA, POS\n",
    "\n",
    "special_case = [{ORTH: u\"bc\", LEMMA: u\"because\", POS: u\"CONJ\"}]\n",
    "nlp.tokenizer.add_special_case(u\"bc\", special_case)\n",
    "\n",
    "suffixes = nlp.Defaults.suffixes + (r'''lb|cm|\\\"''',)\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I'm 5\"5' and 125 lbs. i ordered the s petite to make sure the length wasn't too long. i typically wear an xs regular in retailer dresses. if you're less busty (34b cup or smaller), a s petite will fit you perfectly (snug, but not tight). i love that i could dress it up for a party, or down for work. i love that the tulle is longer then the fabric underneath.\n",
    "\"\"\"\n",
    "textacy.preprocess.preprocess_text(text, False, \n",
    "                                              lowercase=True, no_urls=True, \n",
    "                                              no_emails=True, no_phone_numbers=True, \n",
    "                                              no_numbers=True, no_currency_symbols=True, \n",
    "                                              no_punct=True, no_contractions=False, \n",
    "                                              no_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"it is soooo pretty\"\n",
    "test_str = re.sub(r\"\"\"([a-z])\\1{1,} \"\"\", r\"\"\"\\1 \"\"\", test_str)\n",
    "print(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove unit of measurement\n",
    "    text = re.sub(r'lb[s]?', '', text)\n",
    "    text = re.sub(r'[0-9]{1,2}[\\w]*', '', text)\n",
    "    # repeating words\n",
    "    text = re.sub(r\"\"\"([a-z])\\1{1,} \"\"\", r\"\"\"\\1 \"\"\", text)\n",
    "    # abbreviations\n",
    "    text = re.sub(r'imo', 'in my opinion', text)\n",
    "    text = re.sub(r'bc', 'because', text)\n",
    "    # slang\n",
    "    text = re.sub(r'tad', 'litte', text)\n",
    "    # size\n",
    "    text = re.sub(r'x[x|s]', 'size', text)\n",
    "    \n",
    "    text = textacy.preprocess.preprocess_text(text, False, \n",
    "                                              lowercase=True, no_urls=True, \n",
    "                                              no_emails=True, no_phone_numbers=True, \n",
    "                                              no_numbers=True, no_currency_symbols=True, \n",
    "                                              no_punct=True, no_contractions=True, \n",
    "                                              no_accents=True)\n",
    "    text = re.sub('number', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['clean_text'] = reviews['review_text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(text):\n",
    "    if type(text) is str:\n",
    "        # repeating words\n",
    "        text = re.sub(r\"\"\"([a-z])\\1{1,} \"\"\", r\"\"\"\\1 \"\"\", text)\n",
    "        # abbreviations\n",
    "        text = re.sub(r'imo', 'in my opinion', text)\n",
    "        text = re.sub(r'bc', 'because', text)\n",
    "        # slang\n",
    "        text = re.sub(r'tad', 'litte', text)    \n",
    "        text = textacy.preprocess.preprocess_text(text, False, \n",
    "                                                  lowercase=True, no_urls=True, \n",
    "                                                  no_emails=True, no_phone_numbers=True, \n",
    "                                                  no_numbers=True, no_currency_symbols=True, \n",
    "                                                  no_punct=True, no_contractions=True, \n",
    "                                                  no_accents=True)\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "reviews['clean_title'] = reviews['review_title'].progress_apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_pickle('data/reviews_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_pickle('data/reviews_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Text Matrix Term Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def stemmed_words(doc):\n",
    "    return list(map(stemmer.stem, analyzer(doc)) )\n",
    "\n",
    "pos_interest = ['ADJ','NOUN', 'VERB', 'ADV']\n",
    "def trim_features(doc):\n",
    "    review = nlp(doc)\n",
    "    tokens = []\n",
    "    for t in review:\n",
    "        if not t.is_stop and t.is_alpha and t.pos_ in pos_interest:\n",
    "            t_stem = stemmer.stem(t.text)\n",
    "            if len(t_stem) > 2:\n",
    "                tokens.append(t_stem)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12613 unique lemmas\n",
    "#9694 unique Porter stems\n",
    "#8190 unique Lancaster stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will go with Porter stemming because not much difference to Lancaster while retaining more meaning of words\n",
    "\n",
    "Only considers top 3000 stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "import pickle\n",
    "with open('data/reviews_processed.pkl', 'rb') as f:\n",
    "    reviews = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb61b81084c14e0090275b611818fc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Calculating TF-iDF', max=22641, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words='english',\n",
    "                             max_features=2000,\n",
    "                             min_df=5,\n",
    "                             norm='l1')\n",
    "corpus = list(reviews['clean_text'])\n",
    "\n",
    "X = vectorizer.fit_transform(tqdm(corpus, desc='Calculating TF-iDF', total=len(reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ngram_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ngram_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer.vocabulary_, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate term matrix for title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def stemmed_words(doc):\n",
    "    return list(map(stemmer.stem, analyzer(doc)) )\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words='english',\n",
    "                             analyzer=stemmed_words,\n",
    "                             norm='l1)\n",
    "corpus = list(reviews['clean_title'])\n",
    "\n",
    "X_title = vectorizer.fit_transform(tqdm(corpus, desc='Calculating TF-iDF', total=len(reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/title_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(X_title, f)\n",
    "    f.close()\n",
    "\n",
    "with open('data/title_key.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer.vocabulary_, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reviews.boxplot(column='upvotes', by='star_rating')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
